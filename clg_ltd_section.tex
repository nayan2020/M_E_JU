\chapter{Challenges and Limitations}

{\Large A}lthough the proposed Retrieval-Augmented Generation (RAG) combined with Agentic Artificial Intelligence (AI) framework exhibits notable improvements in flexibility, adaptability, and robustness compared to traditional regular expressions, several challenges and limitations were encountered during the course of research and implementation:

\section{Computational Overhead}
Agentic systems impose significant computational demands, attributable to several factors:

\begin{itemize}
    \item The multi-stage pipeline architecture encompassing retrieval, planning, generation, and execution phases.
    \item Overhead associated with orchestrating multiple tools, particularly when numerous subtasks or agents are activated concurrently.
    \item The deployment of large language models (LLMs), such as GPT-4 Turbo and Gemini-Pro, which contribute to increased latency and operational costs in real-time scenarios.
\end{itemize}

\noindent
\textbf{Implication:} These computational requirements may constrain the deployment of the system in resource-limited environments or latency-sensitive applications unless appropriate optimizations or scaling strategies are employed.



\section{Complexity of Orchestration}
Coordinating interactions among system components—including retrievers, generators, planners, and tool APIs—presents considerable complexity due to the following reasons:

\begin{itemize}
    \item The necessity for precise task decomposition and effective coordination among multiple agents.
    \item The integration of reflection and error handling mechanisms, which further increase implementation complexity.
    \item The risk that failures within one component may propagate through the pipeline if not adequately isolated.
\end{itemize}

\noindent
\textbf{Implication:} This inherent complexity complicates debugging and scaling efforts, making system maintenance more challenging compared to traditional regular expression-based solutions.


\section{Interpretability and Explainability}
While traditional regular expressions provide transparent, rule-based logic, the outputs generated by large language models (LLMs) and agentic systems tend to be less interpretable due to several factors:

\begin{itemize}
    \item Generated patterns or actions often lack explicit justifications or traceability.
    \item Determining the rationale behind specific extractions or decisions can be challenging without comprehensive logging and introspection mechanisms.
\end{itemize}

\noindent
\textbf{Implication:} This opacity may diminish user trust, especially in sensitive application domains such as healthcare or legal analysis, where explainability is critical.


\section{Domain Generalization}
Although the system demonstrates strong performance on synthetic and semi-structured datasets, adapting effectively to diverse real-world domains—such as rule-base agent, financial agent, or legal documents—poses additional challenges:

\begin{itemize}
    \item Necessity for domain-specific retrievers and curated corpora to ensure relevant knowledge retrieval.
    \item Requirement for specialized tool wrappers and tailored extraction strategies aligned with domain conventions.
\end{itemize}

\noindent
\textbf{Implication:} Without domain-specific fine-tuning, the system’s accuracy may degrade, and the likelihood of hallucinations or erroneous extractions may increase when applied across heterogeneous domains.


\section{Human-in-the-Loop Dependency}
The present implementation depends heavily on human evaluators for key functions, including:

\begin{itemize}
    \item Verifying the accuracy of extraction results,
    \item Adjusting system configurations or re-executing tasks that have failed.
\end{itemize}

Moreover, a fully automated feedback mechanism or reinforcement learning loop has not yet been integrated.

\noindent
\textbf{Implication:} This reliance on manual intervention restricts the system’s scalability and precludes full autonomy in continuous deployment or real-time operational environments.




\section{Data Quality and Benchmarking}
Currently, the system utilizes HTML data generated by an external component; however, for improved performance, the data format was converted to JSON. Additionally, there is a notable absence of standardized and diverse benchmarking datasets specifically tailored for pattern extraction using AI agents.

\noindent
\textbf{Implication:} The process of generating high-quality data from appropriate visual elements, alongside ensuring generalization, robustness, and long-term reliability, remains a significant challenge in the evaluation of such systems.










